# Vulnerability_Scanner
Even though all these steps are taken properly there are always some vulnerabilities left behind for which we focus on development of a crawler and vulnerability tester based on fetching all the links and forms available inside a particular URL by capturing all the response and request packets the URL makes and crawling the front end script of the website to fetch all the possible links it either refers or forwards the user too, while testing the same too. Currently the scanner has been developed for crawling through the whole URL and fetching all the links while testing for XSS vulnerable links and forms to which the user provides any PII data.

Working
As per the definition of a crawler, it functions as to collect all the data in the webpage (URL provided by the user) – all the front end code is taken in account, html, CSS, php pages, the syntax in all is read by the processor searching for reference tags, the hyperlinks, which leads to other pages. 
In case extracted data is already in our database it’s discarded to avoid replication and content duplicity, if on some cases the hyperlinks refer to each other in a loop structure they need to be discarded to avoid crashes and overloading. While pre-processing out data extracted, look for the incomplete URLs which are either completed later by the webpage or added to current URL to redirect further.
All this approach provides a path to robust and efficient web crawler, but the security still in question.
All the extracted links are sent to another thread process running parallelly where it’s processed further as-
•	In case if it contains a login process, try brute force login in so that further extraction can be proceeded, providing a better and in depth approach to problem.
•	As all the links are getting extracted they are went to another thread for vulnerability check, where the thread main process can be  developed to allow the check for any type of vulnerability whether it be XSS vulnerability, SQL-injection vulnerability etc.
